sat.seed(1)
set.seed(1)
n <- 100
n
tr <- rbinom(n, 1, 0.5)
tr
tr <- rbinom(n, 4, 0.5)
tr
tr <- rbinom(n, 1, 0.5)
y <- 1 + tr
y
y <- 1 + tr + rnorm(n, 0, 3)
rnomr(n, 0, 3)
rnorm(n, 0, 3)
hist(rnorm(n, 0, 3))
hist(rnorm(n, 0, 1))
hist(rnorm(n, 0, 3))
hist(rnorm(n*5, 0, 3))
hist(rnorm(n*100, 0, 3))
hist(rnorm(n*100, 0, 1))
hist(rnorm(n*100, 0, 3))
hist(rnorm(n, 0, 3))
rnorm(n, 0, 3)
x <- rnorm(n,0,3)
mean(x)
y
new.drp<-numeric(0)
gender<-c(rep("male",10),rep("female",10))
set.seed(999)
height<-round(c(rnorm(10, mean=69.5,sd=3),rnorm(10,mean=64,sd=3)),1)
gender<-c(rep("male",10),rep("female",10))
men.height<-numeric(0)
women.height<-numeric(0)
difference<-numeric(0)
mean(height[1:10]-mean(height[11:20])) #sample difference 3.18
for(i in 1:10000) {
new.height<-sample(height, size=20, replace=FALSE)
men.height<-c(men.height, mean(new.height[1:10]))
women.height<-c(women.height, mean(new.height[11:20]))
difference<-c(difference,mean(new.height[1:10])-mean(new.height[11:20]))
}
# Are the heights of 'men' and 'women' different in the resamples. They shouldn't be.
plot(density(men.height), col='red')
lines(density(women.height), col='blue')
# How big are the simulated height differences?
hist(difference)
# What is the probability of finding a difference of 3.18 or more, just by chance?
options(scipen=999)
sum(difference>3.18)/length(difference)
# Using package perm
library(perm)
permTS(height[1:10],height[11:20],method="exact.mc", alternative="greater")
# Example 2: Directed Reading Activities study
# Example 2: Directed Reading Activities study
observed.diff<-mean(drp$drp[drp$g==0])-mean(drp$drp[drp$g==1])
## Permutation and Randomization Tests""
# Example 1. Generate some data.
set.seed(999)
height<-round(c(rnorm(10, mean=69.5,sd=3),rnorm(10,mean=64,sd=3)),1)
gender<-c(rep("male",10),rep("female",10))
men.height<-numeric(0)
women.height<-numeric(0)
difference<-numeric(0)
mean(height[1:10]-mean(height[11:20])) #sample difference 3.18
for(i in 1:10000) {
new.height<-sample(height, size=20, replace=FALSE)
men.height<-c(men.height, mean(new.height[1:10]))
women.height<-c(women.height, mean(new.height[11:20]))
difference<-c(difference,mean(new.height[1:10])-mean(new.height[11:20]))
}
# Are the heights of 'men' and 'women' different in the resamples. They shouldn't be.
plot(density(men.height), col='red')
lines(density(women.height), col='blue')
# How big are the simulated height differences?
hist(difference)
# What is the probability of finding a difference of 3.18 or more, just by chance?
options(scipen=999)
sum(difference>3.18)/length(difference)
# Using package perm
library(perm)
permTS(height[1:10],height[11:20],method="exact.mc", alternative="greater")
# Example 2: Directed Reading Activities study
observed.diff<-mean(drp$drp[drp$g==0])-mean(drp$drp[drp$g==1])
new.drp<-numeric(0)
difference<-numeric(0)
nrep=10000
for(i in 1:nrep){
new.drp<-sample(drp$drp, size=44, replace=FALSE)
difference<-c(difference,mean(new.drp[1:21])-mean(new.drp[22:44]))
}
# one-sided test p-value
sum(difference>observed.diff)/nrep
#two-sided test p-value
sum(abs(difference)>observed.diff)/nrep
# Example 3: ANOVA with permutation test
library("faraway")
head(coagulation)
#observed results
ggplot(coagulation,aes(x=diet, y=coag))+geom_boxplot()
aov_results = lm(coag ~ diet, data = coagulation)
anova(aov_results)
obt.F = anova(aov_results)$"F value"[[1]] #test statistic
p_val  = anova(aov_results)$"Pr(>F)"[[1]]
nreps<-5000
samp.F <- numeric(nreps)
counter <- 0
for (i in 1:nreps){
new_groups<-sample(coagulation$diet)
new_model<-lm(coagulation$coag ~ new_groups, data = coagulation)
samp.F[i] <- anova(new_model)$"F value"[1]
if (samp.F[i] > obt.F) counter = counter + 1
}
p.value<-counter/nreps
p.value
# Example 4: another ANOVA with permutation test
# Ants and type of bread in Sandwiches
load("Data/SandwichAnts2.rda")
ggplot(SandwichAnts2,aes(x=Bread, y=Ants))+geom_boxplot()
aov_results = lm(Ants ~ Bread, data = SandwichAnts2)
anova(aov_results)
obt.F = anova(aov_results)$"F value"[[1]]
p_val  = anova(aov_results)$"Pr(>F)"[[1]]
nreps<-5000
samp.F <- numeric(nreps)
counter <- 0
for (i in 1:nreps){
new_groups<-sample(SandwichAnts2$Bread)
new_model<-lm(SandwichAnts2$Ants ~ new_groups, data = coagulation)
samp.F[i] <- anova(new_model)$"F value"[1]
if (samp.F[i] > obt.F) counter = counter + 1
}
p.value<-counter/nreps
p.value
# Example 5: coefficient of correlation
load("Data/RestaurantTips.rda")
head(RestaurantTips)
ggplot(RestaurantTips, aes(y=Bill, x=PctTip))+geom_point()
obs.cor=cor(RestaurantTips$Bill, RestaurantTips$PctTip)
obs.cor
sampling.distr=replicate(10000,cor(sample(RestaurantTips$Bill),RestaurantTips$PctTip))
# p-value two-tailed test
sum(abs(sampling.distr)>=obs.cor)/10000
# p-value one tailed test (greater than)
sum(sampling.distr>=obs.cor)/10000
hist(sampling.distr)
R.versio
R.version
R.version
#libraries
library("tidyverse")
library(corrplot) # to visualize correlations
library(ggplot2)
library(Hmisc) # among other things tests on correlation coefficients
library(corrgram) # a different way to visualize correlations
library(ppcor) # partial correlations
library(nortest) # normality tests
library(tseries) # for Jarque Bera's normality test
library(car) # Box-Cox transformations
library(moments) # for kurtosis and skewness
library(MVN) # multivariate normality
library(mvoutlier) # multivariate outliers
library(GGally)
library(rrcovHD)
library(magrittr) #
# Load all datsets at once
load("desc_multiv.RData")
# Load all datsets at once
load("./desc_multiv.RData")
# Load all datsets at once
load("desc_multiv.RData")
#libraries
library("tidyverse")
library(corrplot) # to visualize correlations
library(ggplot2)
library(Hmisc) # among other things tests on correlation coefficients
library(corrgram) # a different way to visualize correlations
library(ppcor) # partial correlations
library(nortest) # normality tests
library(tseries) # for Jarque Bera's normality test
library(car) # Box-Cox transformations
library(moments) # for kurtosis and skewness
library(MVN) # multivariate normality
library(mvoutlier) # multivariate outliers
library(GGally)
library(rrcovHD)
library(magrittr) # needs to be run every time you start R and want to use %>%
# Load all datsets at once
load("desc_multiv.RData")
# Importing data sets
medifis=read.table("medifis.txt")
library("tidyverse")
library(corrplot) # to visualize correlations
library(ggplot2)
library(Hmisc) # among other things tests on correlation coefficients
library(corrgram) # a different way to visualize correlations
library(ppcor) # partial correlations
library(nortest) # normality tests
library(tseries) # for Jarque Bera's normality test
library(car) # Box-Cox transformations
library(moments) # for kurtosis and skewness
library(MVN) # multivariate normality
library(mvoutlier) # multivariate outliers
library(GGally)
library(rrcovHD)
library(magrittr)
# Load all datsets at once
load("desc_multiv.RData")
# Load all datsets at once
load("desc_multiv.RData")
# plotting each point with each final combined weight. Small values indicate
# potential multivariate outliers.
plot(seq(1,100),hb618.out$wfinal)
#libraries
library("tidyverse")
library(corrplot) # to visualize correlations
library(ggplot2)
#libraries
library("tidyverse")
library(corrplot) # to visualize correlations
library(ggplot2)
library(Hmisc) # among other things tests on correlation coefficients
library(corrgram) # a different way to visualize correlations
library(ppcor) # partial correlations
library(nortest) # normality tests
library(tseries) # for Jarque Bera's normality test
library(car) # Box-Cox transformations
library(moments) # for kurtosis and skewness
library(MVN) # multivariate normality
library(mvoutlier) # multivariate outliers
library(GGally)
library(rrcovHD)
library(magrittr) # needs to be run every time you start R and want to use %>%
library("tidyverse") #data manipulation
library(GGally) # nice scatterplot matrix
library(FactoMineR) # PCA computation
library(factoextra) # nice plotting for PCA objects
library(missMDA) # to determine number of PC's through crossvalidation
library(gridExtra) # to build grid of plots
#check you have the correct data set
cereals <- read.table("cerealdata.txt", header=TRUE, as.is=TRUE, na.strings="-1")
cereals1=na.omit(cereals)
library(tidyverse)
library(ppcor)
library("arm")
# Basic output and parameter interpretation for linear models
# loading data
hbat=read.csv("hbat.csv",header=TRUE,sep=",")
# if we have imported the id column as first column, delete it
head(hbat)
hbat=hbat[-1]
# One explanatory variable
ggplot(hbat, aes(x = X20, y = X19)) + geom_point()+geom_smooth(method="lm")
# Separate the response and the explanatory variable
# x20 is the explanatory variable, and x19 is the response variable.
# Simple_1 contains all the information and metadata about the model.
simple_1=lm(X19~X20, data=hbat)
# This is telling you that the model that's fitted is y^= beta^0+beta^1*x
# It provides the coefficients for the beta (fitted model) = 0.78 + 0.8x
# Beta 0 ys not
# Pr(>|t|) is very important to evaluate
# F-statistic: we only have one x variable in this case, so Pr(>|t|) is the same
# than the F-statistic.
# Whenever the p-value is low, the variable is telling you that the regression is helping
# to understand the y (explanatory variable). Thus,
# The f-statistic compares the null model (the one using the mean of y) with the linear
# model.
# H0 = beta1 = 0
# H1 = beta1 != 0 --> The linear regression makes sense, it's helping predict the
# behaviour of y.
# R-squared is the correlation between y and y^. In simple linear regression model,
# cor(X,Y) = cor(Y,Y^). This is because we only have one predictor variable.
# Every time you include a new variable in the model, R-squared either stays the same or
# increases (positively monotonic).
names(simple_1)
summary(simple_1)
# What proportion of the variation in X19 is explained by X20?
cor(hbat$X19,simple_1$fitted)^2 # = to Multiple R-squared
# How is this proportion related to Pearson's correlation coefficient?
cor(hbat$X19,hbat$X20)^2 # = to Multiple R-squared ONLY IN THE SIMPLE LINEAR REGRESION MODEL
hbat_new <- hbat %>%
mutate(fitted = fitted(simple_1),
residuals = residuals(simple_1)) %>% dplyr::select(X19,X20,fitted,residuals)
# Plots the line that minimizes the sums of the square residuals the minimum (check
# formula in notes)
ggplot(hbat_new, aes(x = X20, y = X19)) +
geom_smooth(method = lm, se = FALSE, color = "lightgrey") +
geom_segment(aes(xend = X20, yend = fitted), alpha = .2) +
geom_point() +
geom_point(aes(y = fitted), shape = 2) +
ggtitle("Model residuals for X19 ~ X20")
# Interpreting coefficients:
# Intercept = \hat{\beta_0} represents the predicted average value of X19 when X20 equals 0
# (if this makes sense). Anyway, the high p-value does not reject the null, and it can be assumed
# that the intercept is equal to 0.
# \hat{\beta_1}=0.87 represents the average change in X19 associated with a 1 unit increase
# in the predictor X20. Low p-value in this line means we reject the null that this coeff.
# equals 0.
# Residuals standard error, its square is an estimate for the variance of the residuals
sqrt(sum(simple_1$residuals^2)/98)
# Standardizing both variables. R-square remains the same and the explained variability.
# The only thing that changes is the scale of the coefficients.
data=cbind(hbat[,c(19,20)])
data=data.frame(scale(data, scale=TRUE)) # subtract the meaning and divide by std.
standard=lm(data[,1]~data[,2], data) # the intercept is 0 since both variables have mean 0,
# and \hat{beta_1} is the correlation. The interpretation of \hat{\beta_1} is in terms of
# standard deviations:  subjects giving ratings of X20 one standard deviation above the mean are
# expected to give X19 ratings, on average, 0.76 standard deviations above the mean.
# R-squared remains the same.
summary(standard)
# Introducing a quadratic term, although it is not needed. We can fit a curve instead
# of a line. To prevent artificial collinearity it is a good practice to keep the variable
# just once (for example, keep either x or x^2, but not both of them).
# We can see that we thought using x^2 was going to enhance the model, but the p-value
# shows we fail to reject the null hypothesis (hence we have a simpler model)
# instead of y= b0 + b1x +b2(x-x_mean)^2, we have y= b0 + b1x
ggplot(hbat, aes(x = X20, y = X19)) + geom_point()+geom_smooth(method = lm, formula = y ~ poly(x,2))
quad_1=lm(X19~X20+I(scale(X20, scale=F)^2), hbat)
summary(quad_1)
#I(scale(hbat$X20, scale=F)) == hbat$X20 - mean(hbat$X20)
# Another example where a square term improves the fitting
# data(strongx)
# ggplot(strongx, aes(x = energy, y = crossx)) + geom_point() + geom_smooth(method = lm, formula=y-x, se=FALSE)
#  geom_smooth(method=lm, formula = y-poly(x,2),se=FALSE,color="black", linetype="dotdash")
# quad_2 = lm(crossx-energy+I(scale(energy, scale=FALSE)^2), strongx)
# Two explanatory variables
bi_1=lm(X19~X20+X21, hbat)
summary(bi_1)
# recovering the partial correlation coefficient between YX21.X20
anova(bi_1) #
library(tidyverse)
library(ppcor)
library("arm")
# Basic output and parameter interpretation for linear models
# loading data
hbat=read.csv("hbat.csv",header=TRUE,sep=",")
library(tidyverse)
library(ppcor)
library("arm")
# Basic output and parameter interpretation for linear models
# loading data
hbat=read.csv("hbat.csv",header=TRUE,sep=",")
# if we have imported the id column as first column, delete it
head(hbat)
hbat=hbat[-1]
# One explanatory variable
ggplot(hbat, aes(x = X20, y = X19)) + geom_point()+geom_smooth(method="lm")
# Separate the response and the explanatory variable
# x20 is the explanatory variable, and x19 is the response variable.
# Simple_1 contains all the information and metadata about the model.
simple_1=lm(X19~X20, data=hbat)
library(tidyverse)
library(ppcor)
library("arm")
# Basic output and parameter interpretation for linear models
# loading data
hbat=read.csv("hbat.csv",header=TRUE,sep=",")
reduce([1,2,3], function(x,y) x+y)
reduce(function(x,y) x+y, [1,2,3])
reduce(function(x,y) x+y, rnorm(10))
Reduce(function(x,y) x+y, rnorm(10))
Reduce(function(x,y) x+y, c<-(1,2,3))
Reduce(function(x,y) x+y, (1,2,3))
Reduce(function(x,y) x+y, c(1,2,3))
Reduce(sum, c(1,2,3))
Reduce(sum, c(1,2,3))
sum(c(1,2,3))
mult(c(1,2,3))
mul(c(1,2,3))
Reduce(function(x,y) x+y, filter(c(1,2,3))
9
filter(c(1,2,3), function(x) x<3)
Filter(c(1,2,3), function(x) x<3)
Filter(c(1,2,3), function(x) x<3)
Filter(function(x) x < 3, c(1,2,4))
Reduce(function(x,y) x+y, Filter(function(x) x>1, c(1,2,3))
)
Reduce(function(x,y) x+y, Filter(function(x) x>1, c(1,2,3)))
Reduce(function(x,y) x+y, Filter(function(x) x>1, c(1,2,3,5)))
c(range(10))
range(10)
range(1,10)
list(range(1,10))
n=4000
p=0.001
l = n*p
dpois(0,l)
dpois(0,l) + dpois(1,l)
ppois(0,l) + ppois(1,l)
library(bnlearn)
library(arules)
library(gRbase)
library(gRain)
install.packages("graph")
library(gRbase)
library(gRain)
c(1,2,3,4)[-1]
c(1,2,3,4)[1]
c(1,2,3,4)[-[1]
c(1,2,3,4)[-[1]]
library(keras)
library(tensorflow)
library(dplyr)
library(devtools)
library(tfdatasets)
install_keras()
install_keras()
reticulate::py_install('transformers', pip = TRUE)
devtools::install_github("rstudio/tensorflow")
Sys.which("python")
devtools::install_github("rstudio/keras")
knitr::opts_chunk$set(echo = TRUE)
set.seed(seed)
topic_search <- function(data, word_search, predictions,
sample_size=4, seed=NULL, verbose=F,...){
docs <- data[lapply(data,function(x){grepl(word_search, x, fixed=T)}) %>% unlist(use.names=F)]
if(!length(docs)) stop(paste0("No documents matched for word '", word_search, "'."))
if (seed) set.seed(seed)
docs <- if (length(docs)>sample_size) sample(docs, sample_size) else docs
if(verbose) print(docs)
plot_doc2topic(docs, predictions=predictions)
}
knitr::opts_chunk$set(echo = TRUE)
library(textmineR)
library(quanteda)
library(readr)
library(rlist)
library("spacyr")
source("utils.R")
data_dir <- "data\\"
serial_dir <- paste0(data_dir, "serialized\\")
train_size <- 200000
test_size <- 100000
seed<-0
out_file_train <- paste0(data_dir, "tweets-train-",floor(train_size/1000),"k-prep.txt")
out_file_test <- paste0(data_dir, "tweets-test-",floor(test_size/1000),"k-prep.txt")
print(out_file_train)
print(out_file_test)
input_file <- paste0(data_dir,"tweets.csv")
set.seed(seed)
df.tweets <- sample(read.csv(input_file, header=FALSE)$V6)
df.tweets.train <- df.tweets[1:train_size]
df.tweets.test <- df.tweets[train_size+1: (test_size+1)]
# Liberate memory space
rm(df.tweets)
head(df.tweets.train, 10)
df.corpus <- iconv(corpus(df.tweets.train), from = "UTF-8", to = "ASCII", sub = "")
df.tokens <- tokens(df.corpus,
remove_punct = TRUE,
remove_url = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE
) %>% tokens_tolower(
) %>% tokens_select(
pattern = stopwords("en"), selection = "remove"
) %>% tokens_select(
pattern = "@\\S+", selection="remove", valuetype ="regex"
) %>% tokens_select(
pattern = "www\\.\\S+", selection="remove", valuetype ="regex"
) %>% tokens_select(
pattern = "amp", selection="remove"
) %>% tokens_select(
pattern = "quot", selection="remove"
)
head(df.tokens, 5)
spacy_initialize(model = "en_core_web_sm")
df.spacy <- lapply(df.tokens, reduce) %>% unlist(use.names = F) %>% spacy_parse
spacy_finalize()
head(df.spacy, 10)
df.processed <- aggregate(df.spacy$lemma, list(df.spacy$doc_id), FUN=reduce)$x
head(df.processed, 10)
getwd()
setwd("C:/Users/David/Desktop/nlp-project")
df.processed <- aggregate(df.spacy$lemma, list(df.spacy$doc_id), FUN=reduce)$x
head(df.processed, 10)
if (file.exists(out_file_train)){
df.processed <- read.delim(out_file_train, header=F)$V1
} else {
write_lines(df.processed, out_file_train)
}
tcm_filename <- paste0(serial_dir, "tcm_tweets.dat")
if (file.exists(tcm_filename)){
tcm <- list.unserialize(tcm_filename)
} else{
tcm <- CreateTcm(doc_vec = df.processed,
#skipgram_window = 3,
verbose = FALSE,
cpus = 4)
list.serialize(tcm, tcm_filename)
}
library(textmineR)
library(quanteda)
library(readr)
library(rlist)
library("spacyr")
source("utils.R")
