---
title: "Topic Modeling on the tweets dataset"
author: "David Lorenzo Alfaro"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
The increasingly overwhelming amount of available natural language motivates the pressing need to find efficient and reliable computational techniques capable of processing and analysing this type of data to achieve human-like natural language understanding for a wide range of downstream tasks. Over the last decade, Natural Language Processing (NLP) has seen impressively fast growth, primarily favoured by the increase in computational power and the progress on unsupervised learning in linguistics.

Dealing with unannotated data poses as an unsupervised learning problem, a paradigm within Machine Learning in which no outputs or target variables are provided, i.e., the model is only trained with inputs x, $D=\{x_i\}_{i=1}^N$, with the goal being to discover or extract patterns present in data. Often referred to as knowledge discover in the literature, this class of problem is much less defined and since it lacks from ground truth, it is not trivial to find efficient metrics to use for the model to learn about relationships in data and to assess goodness of the identified groups.

In this assignment, we use Latent Dirichlet Allocation (LDA) (Blei et al., 2003), a generative probabilistic model for collections of discrete data such as text corpora in which each item is modelled as a finite mixture over an underlying set of topics. In this context, we attempt to obtain meaningful representations of documents according to the topic probabilities yielded by a LDA model.

## Problem description

For this work we will use a dataset composed of 1.6 million tweets,  first introduced (Go et al., 2009). Originally proposed for sentiment classification, in this assignment we are mainly concerned with the text of the messages and not in the label, from which the most prominent topics will be captured via LDA.

## Import all dependencies

Check whether all necessary packages are installed, and install those that are not automatically from the `CRAN` repository.

```{r, results=FALSE}
source("requirements.R")
```

Packages `quanteda` and `spacyr` are used for preprocessing purposes, `textmineR` to induce a topic model from data using LDA and to perform queries at inference time; packages `readr` and `rlist` to serialize information allowing to load (store) data from disk to provide with faster execution times by avoiding repeating calculations; and `plotly`, used to generate interactive graphs to visualize topic assignations for words and documents.

```{r, results=FALSE}
library(textmineR)
library(quanteda)
library(readr)
library(rlist)
library("spacyr")
source("utils.R")

```

## Load data

Let us first define some variables to handle data directories:

-   `data_dir` holds the relative path where all data is stored.

-   `serial_dir` is the relative path where all serialized data is stored, allowing for faster execution by eschewing recomputing some key components.

```{r}
data_dir <- "data\\"
serial_dir <- paste0(data_dir, "serialized\\")
```

Next, we define the size of the training and test sets, composed of $2·10^5$ and $10^5$ instances, respectively, where an instance is a preprocessed tweet (i.e., not in raw form).

```{r}
train_size <- 200000
test_size <- 100000
```

Similarly, in order to grant reproducibility of all experiments hereby conducted we set a seed. This will induce deterministic execution of directives using randomness (e.g., the `sample` function).

```{r}
seed <- 0
```

Let us generate the subsequent output filenames for both the training and test set. As aforementioned, this will come handy to avoid repeating preprocessing steps that should provide a deterministic execution, due to the static nature of the data.

```{r}
out_file_train <- paste0(data_dir, "tweets-train-",floor(train_size/1000),"k-prep.txt")
out_file_test <- paste0(data_dir, "tweets-test-",floor(test_size/1000),"k-prep.txt")
print(out_file_train)
print(out_file_test)
```

In order to generate the training and test datasets in an unbiased manner:

1.  Load the whole dataset and obtain a (random) permutation. Randomness can be controlled via a specific random state.

2.  Obtain the training dataset, composed of `train_size` instances, denoted $data_{training}=\{x_1, ..., x_i, ..., x_{train\_size}\}$

3.  Obtain the test dataset, composed of `test_size` instances, denoted $data_{test}=\{x_1, ..., x_i, ..., x_{test\_size}\}$

4.  In order to leverage memory management, remove the `df.tweets` object (no longer necessary).

```{r}
input_file <- paste0(data_dir,"tweets.csv")
set.seed(seed)
df.tweets <- sample(read.csv(input_file, header=FALSE)$V6)
df.tweets.train <- df.tweets[1:train_size]
df.tweets.test <- df.tweets[train_size+1: (test_size+1)]

# Liberate memory space
rm(df.tweets)

head(df.tweets.train, 10)
```

## Preprocessing

Tweets are normally written in an informal register Thus, the data preparation phase is arguably the most challenging one, entailing standardization of the language, dealing with slang words and phrases and wrong grammar and spelling, to name a few. In this work we propose a rather naive workflow to deal with the heterogeneous nature of the data, albeit benefiting from further preprocessing could potentially enhance performance of the subsequent models.

### Tokenization

Preprocessing is largely conducted at word level. We utilize `quanteda` to *tokenize* tweets, i.e., decomposing an arbitrarily long sentence or text into its constituent words and/or symbols.

First, we define a corpus from the original training set. Note that we impose texts to be encoded in ASCII, removing non-ASCII characters like those refering to emojis or non-English graphemes.

Next, sentences are broken down into tokens. Illegal tokens will be filtered out according to the following criteria:

1.  `remove_punct`: removes punctuation characters, e.g., "!", ".", ";".

2.  `remove_url`: eliminates URLs beginning with http(s)

3.  `remove_symbols`: removes special symbols.

4.  `remove_numbers`: deletes tokens that consist only of numbers, but not words that start with digits, e.g., `2day`.

5.  `tokens_tolower()`: convert all tokens to lower case.

6.  Remove English stop words, yielded by invoking the `stopwords("en")` function, where "en" stands for the language code.

Furthermore, we delete the ampersand (&) and quot symbols, and define some regular expressions to delete usernames (`@\\S+`), and URLs starting with "www" (`www\\.\\S+`)

------------------------------------------------------------------------

***Note***: the `quanteda` library provides a directive which remove Twitter characters `@` and `#`, denoted `remove_twitter`. We do not use it here because we deal with such symbols in a more meaningful fashion.

```{r}
df.corpus <- iconv(corpus(df.tweets.train), from = "UTF-8", to = "ASCII", sub = "")

df.tokens <- tokens(df.corpus, 
                    remove_punct = TRUE, 
                    remove_url = TRUE, 
                    remove_symbols = TRUE,
                    remove_numbers = TRUE
                    ) %>% tokens_tolower(
                    ) %>% tokens_select(
                      pattern = stopwords("en"), selection = "remove"
                    ) %>% tokens_select(
                      pattern = "@\\S+", selection="remove", valuetype ="regex"
                    ) %>% tokens_select(
                      pattern = "www\\.\\S+", selection="remove", valuetype ="regex"
                    ) %>% tokens_select(
                      pattern = "amp", selection="remove"
                    ) %>% tokens_select(
                      pattern = "quot", selection="remove"
                    )

head(df.tokens, 5)
```

SpaCy is an open source and state-of-the-art performant library that features what they call *linguistically-motivated tokenization*. In this work, we aim at finding topics on tweets written in English. We chose the `en_core_web_sm` language model, which provides simple and fast preprocessing pipelines, with nearly negligible accuracy differences with respect to the so-called *large* model, `en_core_web_trf`.

```{r}
spacy_initialize(model = "en_core_web_sm")
```

The `spacy_parse` function calls spaCy to both tokenize and tag the texts, and returns a `data.table` of the results. Notice that, prior to feeding the parser with inputs, we assemble tokens into texts. We found by empirical experimentation that parsing text in this manner provides faster computations, probably because fewer invocations to the parser are required.

According to the package documentation, while running spaCy on Python through R, a Python process is always running in the background and `Rsession` will take up a lot of memory (typically over 1.5GB). In order to free up the memory allocated to such process, we call the `spacy_finalize` function.

```{r}
df.spacy <- lapply(df.tokens, reduce) %>% unlist(use.names = F) %>% spacy_parse
spacy_finalize()
```

```{r}
head(df.spacy, 10)
```

Lemmatisation in linguistics refers to the process of gathering altogether the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. In order to reduce the number of tokens effectively, we dump all meanings of formal verbs and nouns into their dictionary form.

```{r}
df.processed <- aggregate(df.spacy$lemma, list(df.spacy$doc_id), FUN=reduce)$x

head(df.processed, 10)

```

Preprocessing the input is a costly process, even for a rather small dataset. The most straightforward to alleviate computational needs, one can serialize information, allowing for loading (or storing) data from disk whilst maintaining all meaningful info. In this case, it can be convenient to keep the processed tweets in a human-readable format, hence we simply sink the tweets into a `txt` file using the `write_lines` function of the `readr` package. Similarly, to load data from, we use the `read.delim` function.

```{r}
if (file.exists(out_file_train)){
  df.processed <- read.delim(out_file_train, header=F)$V1
} else {
  write_lines(df.processed, out_file_train)
}
```

Further, the `utils` model features some functions that enables applying this very processing pipeline. Namely, they are the `preprocessing` and the `tokenize` directives. As the name suggests, the former covers all the preprocessing process, whereas the later is solely concerned with the tokenization process. Refer to the code for further details, should you require it.

## Model induction via LDA.

Latent Dirichlet allocation constitutes one of the main approaches for topic modeling. It is roughly based on these two principles:

-   *Every document is a mixture of topics*. A document may contain words belonging for several topics. LDA assigns documents to topics in a probabilistic fashion, i.e., membership to topics is inferred from the likelihood estimates.

-   *Every topic is a mixture of words*. A topic can be characterized by a set of words. For example, given the topic of "entertainment", one could think of words such as "music", "singer", "actress", "movie", etc. A word may belong to more than one topic.

This allows documents to interact with each other, i.e., they can share semantics, rather than being segmented into mutually-exclusive groups (e.g., a binary classification problem), in a way that resembles the organic use of natural language.


### Term co-ocurrence matrix.

A term co-ocurrence matrix (TCM) is a square matrix whereby rows and columns are characterized by each of the tokens in the vocabulary of the corpus. There are to main approaches:

1.  **term-context matrix**, where the whole document serves as a context. If two terms occur in the same context, they are said to have occurred in the same occurrence context.

2.  **k-skip-n-gram** approach, where a sliding window will include the (k+n) words, both to the left and the right. This window will serve as the context now. Terms that co-occur within this context are said to have co-occurred.

Because length of the tweets are bounded, we chose the first approach, which slightly reduces the overhead introduced by the windowing functions. This is a particular case of the skip gram model, where the size of the window equals the size of the document.

The TCM is based on the rationale that terms that appear conjointly across the corpus are prone to be semantically related. Of course, removal of stop words avoids appearance of spurious correlations that may arise. Similarly, in order to better grasp those relations (and to reduce complexity of the matrix), the lemmatisation dumps all forms of a verb/noun into its dictionary form.

As the cardinality of the vocabulary increases, correlations between words tend to be more marginal, in the sense that out of all the words, only a small proportion of them are meaningfully related. Therefore, sparse representations are more suitable (the equivalent dense representations have a quadratic order of space complexity). In this problem, we have a vocabulary size of 68984 tokens, which leads to a TCM with 4,758,792,256 entries. As for the preprocessed dataset, we serialize it to reduce the massive computational overhead.

```{r}
tcm_filename <- paste0(serial_dir, "tcm_tweets.dat")

if (file.exists(tcm_filename)){
  tcm <- list.unserialize(tcm_filename)
} else{
  tcm <- CreateTcm(doc_vec = df.processed,
                   #skipgram_window = 3,
                   verbose = FALSE,
                   cpus = 4)
  list.serialize(tcm, tcm_filename)
}
```

### Fitting the model.

Once we have a TCM, we can use the same procedure to make an embedding model. It is worth noting that this process is very costly because of dimensionality of the matrix. We fit a LDA model from the data with the following hyperparameters:

-   `dtm` : document term matrix or term co-occurrence matrix of class dgCMatrix.

-   `iterations` (200): Integer number of iterations for the *Gibbs* sampler to run.

-   `burnin` (): Integer number of burnin iterations. If `burnin` is greater than -1, the resulting "phi" and "theta" matrices are an average over all iterations greater than `burnin`

-   `alpha` (): Vector of length `k` for asymmetric or a number for symmetric. This is the prior for topics $T = \{t_1, ..., t_n\}$ over documents $D=\{d_1, …, d_n\}$, i.e., $\{P(t_i|d_i) \forall t_i \in T \wedge d_i \in D\}$.

-    `beta` (): Vector of length `ncol(dtm)` for asymmetric or a number for symmetric. This is the prior for words $W=\{w_1, ...., w_n \}$ over topics $T = \{t_1, ..., t_n\}$, i.e., $\{P(w_i|T) | w_i \in W\}$.

-   `optimize_alpha` (`True`): adjust $\alpha$ every 10 *Gibbs* iterations.

-   `calc_coherence` (`True`): calculate probabilistic coherence of topics after the model is trained.

-   `calc_r2` (`True`): calculate R-squared after the model is trained.

-   `cpus` (4): number of CPUs to use to compute the model.

```{r}
k_topics <- 20
embeddings_filename <- paste0(serial_dir, "lda_model_k", k_topics, ".dat")

if (file.exists(embeddings_filename)){
  embeddings <- list.unserialize(embeddings_filename)
} else{
  print(embeddings_filename)
  embeddings <- FitLdaModel(dtm = tcm,
                          k = k_topics,
                          iterations = 200,
                          burnin = 175,
                          alpha = 0.1,
                          beta = 0.05,
                          optimize_alpha = TRUE,
                          calc_coherence = TRUE,
                          calc_r2 = TRUE,
                          cpus = 4)
  
  list.serialize(embeddings, embeddings_filename)
}
```

R-squared ($R^2$) comes handy to get an estimate of the overall goodness of fit of the model. A possible interpretation of this metric is as the amount of variance explained by the model respect to the original term co-ocurrence matrix. A property verified by R-squared is that $0\leq R^2 \leq 1$, where $R^2=1$ denote a perfect fit, hence values closer to 1 are preferrable. In this context, a very low value of $R^2$ may reveal (1) deficiencies in the training process, e.g., because of bad election of the model hyperparametrization, mainly due to suboptimal number of the topics to identify or the number of iterations to perform; or (2) absence of correlations in data, which does not allow for pattern identification.

The yielded value for R-squared is $R^2= 0.304$, which far from being ideal, largely enhances the one obtained without any preprocessing ($\sim 0.113$). It is worth noting that, in this scenario, the *optimal* number of topics, denoted $k$, is likely to be larger than 20 (remember that the cardinality, denoted $|·|$, of the training set is $|data_{training}|=2·10^5$). That notwithstanding, large values for $k$ can derive in computationally intractable problems and increase the complexity to interpret the topics.

```{r}
# Get an R-squared for general goodness of fit
embeddings$r2

```
Now, let us retrieve the top $k$ most representative terms of each topic, using the `GetTopTerms` function, which takes as input the matrix containing the likelihood for words to belong to each of the topics (`embeddings$phi`) and the number of top terms to extract, `top_k_terms`.

Then, we create a summary including the name of the topics and the prevalence, coherence and top $k$ terms of each of the topics.

```{r}
top_k_terms = 5
# Get top terms, no labels we only have unigrams (words treated as a unique entity).
embeddings$top_terms <- GetTopTerms(phi = embeddings$phi,
                                    M = top_k_terms)


embeddings$summary <- data.frame(
  topic = rownames(embeddings$phi),
  prevalence = round(colSums(embeddings$theta), 3),
  coherence = round(embeddings$coherence, 3),
  top_terms = apply(embeddings$top_terms, 2, paste, collapse=", "),
  stringsAsFactors = FALSE
  )

# alternatively, you can use the ldaSummary function:
# embeddings$summary <- ldaSummary(embeddings)
```

Let us have a look to the summary, focusing on the topics ordered by prevalence, which measures the density of tokens along each embedding dimension (i.e., each topic).

```{r}
printSummaryTable("prevalence")
```


Similarly, we can order the table by topic coherence.

```{r}
printSummaryTable("coherence")
```

### Global predictions

After inspecting the model, a naive method to test whether the topics are able to convey the semantic information described by the top terms is by asking the model to classify other related terms. A constraint to this approach is that the *query* terms should belong to the vocabulary of the model, similar to all approaches treating words as "indices", rather than a set of features (semantic-driven embeddings).

To that end, let us first predict the membership likelyhood to each of the topics for all words in the vocabulary.

```{r}
word_predictions_filename = paste0(serial_dir, "word_predictions_k", k_topics, ".dat")

# predict on held-out documents using gibbs sampling "fold in"
df.processed.predictions <- predict_word(embeddings,
                                         tcm,
                                         out=word_predictions_filename,
                                         method = "gibbs",
                                         iterations = 200, 
                                         burnin = 175)

```

Afterwards, let us test some words. This is the behaviour that we expect from the model:

-   Word `college` can be semantically related to topic 7 (feel, bad, school, hate, sick), and to a lesser extent to topic 20 (back, home, week, work, leave) and topic 9 (work, night, time, sleep, bed)

-   Word `tired` can be semantically associated to topic 9 (work, night, time, sleep, bed).

-   Word `lunch` can be linked to topic 18 (eat, make, food, good, dinner).

-   Word `android` can be semantically associated to topic 6 (work, find, iphone, phone, update).

-   Word `disease` can be related to topic 3 (hurt, feel, hair, bad, back).

```{r}
plot_words2topic(c("college", "tired", "lunch", "android", "disease"),
                 df.processed.predictions)
```
As it can be seen from the graphs, all our hypothesis are satisfied, i.e., the model is able to leverage relations among words and topics based on the frequency of appearance in the same context.

## Working on the test dataset: inference.

In this section, we embed documents under the LDA model to obtain topics of **local** contexts, rather than global ones.

First, let us first preprocess the test set.

```{r}
if (file.exists(out_file_test)){
  df.processed.test <- read.delim(out_file_test, header=F)$V1
} else {
  df.processed.test <- preprocess(df.tweets.test)
  write_lines(df.processed.test, out_file_test)
}
```

Then, let's check whether the tweets seem to be properly preprocessed.

```{r}
set.seed(seed)
sample(df.processed.test, 6)
```

In this case, we are interested in modelling each document as a mixture of topics. To examine the per-document-per-topic probabilities, we first obtain a document-term matrix, which describes the frequency of terms that occur in a collection of documents, i.e., a standard bag-of-words representation.

```{r}
dtm.test <- getDtm(df.processed.test)
```

Then, we project the documents into the embedding space by predicting on held-out documents, using *Gibbs* sampling.

```{r}
doc_predictions_filename <- paste0(serial_dir, "doc_predictions_k", k_topics, ".dat")

df.processed.test.predictions <- predict_doc(embeddings,
                                             dtm=dtm.test,
                                             out=doc_predictions_filename
                                             )
```

Now, let us get four random documents from the test set containing the word "college". Notice that the documents shown here are the preprocessed tweets, not the original, and thus they may not be very interpretable.

```{r}
word_search<-"college"
search_sample <- 4

set.seed(seed)
doc.search <- df.processed.test[
                lapply(df.processed.test,
                       function(x){grepl(word_search, x, fixed=T)}
                       ) %>% unlist(use.names=F)
                ] %>% sample(search_sample)

doc.search
```

What is the distribution of the documents respect to the topics? Are they any meaningful?

```{r}
plot_doc2topic(doc.search, predictions=df.processed.test.predictions)
```

As it can be seen, most of the documents seem to be highly related to topic 20, characterized by words back, home, week, work and leave. We argue the model is capable of grasping the overall meaning of the documents, referring to going back to college ( preprocessed tweet: since three month old go college now back year now), doing work for college ("back college least make whether work going do another matter") and having a job interview to afford college expenses ("wooo job interview awesome opportunity heart set travel summer need money college").

On the other hand, the model reveals tweet "sit home reply hundred message multiply good news college english class final" is related with topics 7 (feel, bad, school, hate, sick) and topic 4 (twitter, follow, tweet, read, send). Only the latter seems to make sense, as it includes related words such as "reply" or "message".

<br>

Now, let us retrieve some random documents

```{r}
set.seed(16)
doc_random_search <- sample(df.processed.test, search_sample)
plot_doc2topic(doc_random_search, predictions=df.processed.test.predictions)
```
The model is able to capture salient information about the documents, whenever they can be somehow linked to a topic, whereas for those that cannot be associated clearly to any topic the per-topic probability estimates are uniformly distributed.

When querying for tweets containing "Harry Potter", the model identifies the main topic is topic 17 (watch, movie, sad, love, tv).
```{r}
topic_search(df.processed.test, "harry potter", df.processed.test.predictions, seed=seed)
```
When querying for documents containing the word purchase, results are again meaningful:

-   The first tweet contains the word "wait", related with topic 2 (nt, ca, wait, happy, birthday) and the word "home", related with topic 20 (back, home, week, work, leave)

-   The second tweet contains "browse" related with topic 6 (work, find, iphone, phone, update) and "purchase", with topic 12 (buy, day, year, pay, money).

-   The third tweet contains words "purchase" and "buy", associated with topic 12.

-   The fourth tweet contains the word "page", related with topic 6, and words "store", "purchase" and "checkout", related with topic 12.
```{r}
topic_search(df.processed.test, "purchase", df.processed.test.predictions, seed=seed)
```
## Conclusion
Throughout this work we have found LDA to be very effective onto finding prominent topics in vast amounts of raw, unannotated data. However, because words are treated as mere indices, the model fails to resolve ambiguities inherent to natural language (e.g., lexical, syntactic, semantic and pragmatic ambiguities).


Furthermore, the notion of similarity between words is solely based on words appearing in similar contexts, and not in the intrinsic meaning of each word (e.g., the semantic information a word conveys, given the context in which it appears), where Neural Natural Language Processing, which favours from using Deep Learning models to learn language representations, poses as a better alternative, e.g., BERT (Devlin et al., 2018), ELMo (Peters et al., 2018) and some off-the-likes.


## References
Blei, D. M., Ng, A. Y., & Edu, J. B. (2003). Latent Dirichlet Allocation. *Journal of Machine Learning Research, 3*(Jan), 993–1022.

Go, A., Bhayani, R., & Huang, L. (2009). Twitter Sentiment Classification using Distant Supervision. *CS224N Project Report, Stanford, 1*(2009), 1–12.

## Annex I. Session information.

```{r}
sessionInfo()
```
